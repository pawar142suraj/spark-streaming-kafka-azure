# Apache-Spark-and-Databricks-Stream-Processing-in-Lakehouse
This is the central repository for all the materials related to <em>Apache Spark and Databricks Stream Processing in Lakehouse</em> <br>Course by Prashant Kumar Pandey.
</a>


<h2> Description </h2>
<p align="justify">
  I am creating Apache Spark and Databricks - Stream Processing in Lakehouse using the Python Language and PySpark API. This will help you understand Real-time Stream processing using Apache Spark and Databricks Cloud and apply that knowledge to build real-time stream processing solutions. This is example-driven and follows a working session-like approach. 
</p>

<h3>Capstone Project</h3>
<p align="justify">
This course also includes an End-To-End Capstone project. The project will help you understand the real-life project design, coding, implementation, testing, and CI/CD approach. 
</p>

<h3>Audience</h3>
<p align="justify">
This is for software engineers willing to develop a Real-time Stream Processing Pipeline and application using Apache Spark. I am also creating this course for data architects and data engineers who are responsible for designing and building the organizationâ€™s data-centric infrastructure. Another group of people is the managers and architects who do not directly work with Spark implementation. Still, they work with those implementing Apache Spark at the ground level.
</p>

<h3>Spark and source code version</h3>
<p align="justify">
This Course is using the Apache Spark 3.5. I have tested all the source code and examples used in this Course on Azure Databricks Cloud using Databricks Runtime 14.1.
</p>

</div>
